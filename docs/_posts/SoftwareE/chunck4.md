# How Money Works

Since 1971, under Richard Nixon’s Republican period, the dollar stopped having an amount of gold standing behind it. From that year, all around the world, money stopped being something with a material meaning and started to be the reflection of trust in a system of government and private entities (national and private banks among them, but not the only ones). National banks can decide to create more money papers or bills (dollars, euros, pesos), More papers , same trust, more inflation. seems like a lot of power concentrated in a couple of hands rigth ?

## What Is Proof of Work
### What Is an Algorithm
It is just a list of instructions that leaves no room for ambiguity. The language or structures that allow you to create a set of instructions like this, so a PC can interpret them, are normally known as a programming language.

## What and Why Is a Block

### What Is a Hash

### What Is Bitcoin

The popularization of these technologies and the massification of the use of LLM-based applications like ChatGPT, together with the necessity of less battery consumption, have made necessary changes in chip architectures. Becoming more popular is what is known as “system on a chip,” which is the integration on a single chip of the CPU, TPU, GPU, NPU, Wi-Fi adapter, barometers, thermometers, etc.  
I know, I know — in this series we have only covered a couple of units so far, so let’s review.

## CPU
The most common processing unit. It processes things in series and switches between processes to give a sensation of parallelism.  
Intel, AMD, Snapdragon, M1 from Apple… Every operation occurs in what is called a cycle, controlled via a little crystal inside the PC.  
A 2.1 MHz frequency means 2.1 million cycles per second. An operation — the smallest one — needs about 4 cycles (just to save something like a number in memory). A click or writing a letter is a compilation of these small operations.

## TPU
A Tensor Processing Unit is a chip designed to do tensor math, where a tensor is a special kind of number array, like a matrix but with more than two dimensions. It uses 8-bit arithmetic to be precise and energy-saving. It is especially used for neural networks. It was invented by Google and has its own low-level programming language called CUDA. Imagine it like a monk that only does linear algebra.

## GPU
A GPU uses parallelism in processing; it does not just emulate it like a CPU would. GPUs were developed especially because of the gaming industry. They usually have their own RAM memory called VRAM and their own cooling system.  
We still use CPUs because GPUs are awful with general tasks. Imagine the CPU as a manager, great with multitasking (which is actually changing focus between tasks quickly and easily), and the GPU as a painter — great at one thing, but nothing else — with a bunch of arms to use.

## NPU
A Neural Processing Unit is normally embedded in cell phones. They are used to do voice recognition, face recognition, and image recognition without having to connect to a neural network over the internet.
